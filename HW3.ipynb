{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZLC7eWQQ7v"
      },
      "source": [
        "## Homework 3: Sentiment Classification\n",
        "\n",
        "**Jackson Rudoff**\n",
        "\n",
        "*April 17, 2023*\n",
        "\n",
        "[Github Repo](\"https://github.com/jer164/sst-competition\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Data, the Models, and Why Bother?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this week's project, we're looking into ways to model sequential data. The first examples we discussed were, of course, things like video and audio, which have a highly time-oriented structure and can be divided into slices quite easily.\n",
        "\n",
        "Text classification may not necessarily be something we think of as being inherently sequential. We could, for example, build some decent predictive models that just focus on texts as purely a bag of words, and try to extract meaningful sentiment information with those words as individual units. But words don't really happen in isolation, natural language occurs as a stream, meaning that we may be able to get better predictions by representing this language sequentially. Of course, it is also entirely possible that the sequence doesn't reveal much about the semantics; however, it's definitely worth trying, as there may be some multi-word patterns that are especially valuable for prediction.\n",
        "\n",
        "With this in mind, let's look at the data that we're working with. First we'll read in the ```aimodelshare``` library and download the competition data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y9MfdqKP9el",
        "outputId": "42228db7-6fae-41e4-8fc1-bd242ff62940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: aimodelshare==0.0.189 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (0.0.189)\n",
            "Requirement already satisfied: botocore==1.29.82 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.29.82)\n",
            "Requirement already satisfied: scikit-learn==1.2.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.2.1)\n",
            "Requirement already satisfied: docker==5.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (5.0.0)\n",
            "Requirement already satisfied: pydot==1.3.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.3.0)\n",
            "Requirement already satisfied: shortuuid>=1.0.8 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.0.11)\n",
            "Requirement already satisfied: seaborn>=0.11.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aimodelshare==0.0.189) (0.11.2)\n",
            "Requirement already satisfied: tf2onnx in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.13.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.6.3)\n",
            "Requirement already satisfied: onnxruntime>=1.7.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.13.1)\n",
            "Requirement already satisfied: onnxmltools>=1.6.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.11.1)\n",
            "Requirement already satisfied: pathlib>=1.0.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.0.1)\n",
            "Requirement already satisfied: protobuf==3.19.6 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (3.19.6)\n",
            "Requirement already satisfied: scipy==1.7.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.7.0)\n",
            "Requirement already satisfied: psutil>=5.9.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (5.9.2)\n",
            "Requirement already satisfied: regex in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (2022.10.31)\n",
            "Requirement already satisfied: onnx==1.12.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.12.0)\n",
            "Requirement already satisfied: keras2onnx>=1.7.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.7.0)\n",
            "Requirement already satisfied: tensorflow==2.9.2 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (2.9.2)\n",
            "Requirement already satisfied: Pympler==0.9 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (0.9)\n",
            "Requirement already satisfied: PyJWT>=2.4.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (2.6.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.13.1)\n",
            "Requirement already satisfied: importlib-resources==5.10.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (5.10.0)\n",
            "Requirement already satisfied: boto3==1.26.69 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.26.69)\n",
            "Requirement already satisfied: skl2onnx>=1.14.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.14.0)\n",
            "Requirement already satisfied: onnxconverter-common>=1.7.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (1.13.0)\n",
            "Requirement already satisfied: wget==3.2 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from aimodelshare==0.0.189) (3.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (0.37.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (1.16.0)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3==1.26.69->aimodelshare==0.0.189) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from boto3==1.26.69->aimodelshare==0.0.189) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from botocore==1.29.82->aimodelshare==0.0.189) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore==1.29.82->aimodelshare==0.0.189) (1.26.11)\n",
            "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker==5.0.0->aimodelshare==0.0.189) (2.28.1)\n",
            "Requirement already satisfied: pywin32==227 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from docker==5.0.0->aimodelshare==0.0.189) (227)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker==5.0.0->aimodelshare==0.0.189) (0.58.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-resources==5.10.0->aimodelshare==0.0.189) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx==1.12.0->aimodelshare==0.0.189) (4.3.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from onnx==1.12.0->aimodelshare==0.0.189) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from pydot==1.3.0->aimodelshare==0.0.189) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (1.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.9.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.9.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (21.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (15.0.6.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.30.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.51.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.7.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.12)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (63.4.1)\n",
            "Requirement already satisfied: fire in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from keras2onnx>=1.7.0->aimodelshare==0.0.189) (0.5.0)\n",
            "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.10.1)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (15.0.1)\n",
            "Requirement already satisfied: pandas>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (1.4.4)\n",
            "Requirement already satisfied: matplotlib>=2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (3.5.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare==0.0.189) (4.25.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare==0.0.189) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare==0.0.189) (9.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare==0.0.189) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn>=0.11.2->aimodelshare==0.0.189) (2022.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2022.9.14)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.16.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.0.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.3.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from coloredlogs->onnxruntime>=1.7.0->aimodelshare==0.0.189) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.3.1)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.7.0->aimodelshare==0.0.189) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "### Read in modelshare library\n",
        "\n",
        "! pip install aimodelshare==0.0.189"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKvaHDpXRgG7",
        "outputId": "12bdf173-beb3-413f-a49c-5fb393b8b607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Singer/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Borrow code from example notebook to get workable data\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
        "\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "# Set up one-hot encoding\n",
        "y_train = pd.get_dummies(y_train_labels)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at a few observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\n",
            "The production has been made with an enormous amount of affection , so we believe these characters love each other .\n",
            "This documentary is a dazzling , remarkably unpretentious reminder of what ( Evans ) had , lost , and got back .\n"
          ]
        }
      ],
      "source": [
        "print(X_train[4])\n",
        "print(X_train[67])\n",
        "print(X_train[445])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, our observations are not *super* long, and generally don't have a lot of words for us to extract information from. This is perhaps why a sequential approach might be useful, as it provides an additional dimension to accommodate what are relatively brief extracts.\n",
        "\n",
        "\n",
        "The question remains, what is valuable about a model like this? To begin with, the most obvious and related example would be a review aggregation site. It's possible to do this already using reviewers' provided scores, but the inherent flaw in this system is that people usually provide their own rating arbitrarily. Sites like Letterboxd, for example, are saturated with reviews that are hyperbolic or intended to be comedic, with users attaching 1 or 2 stars based on the point they want to make. But sites like this could easily benefit from having an in-house model that can provide a \"true\" negative/positive rating based on a classification of the actual review, rather than just the number of stars a user enters. This could maybe help ward off issues like review bombing and trolling, and give site users a secondary rating that goes beyond the typical, skewed star-based ratings.\n",
        "\n",
        "Generally, these text-based models are extremely useful for tasks involving language assessment. A sentiment classifier can be very useful for research in the field of linguistics. One of the major challenges in the study of language is the difficulty in coding speech beyond just the words a subject uses. If you are trying to do research on how respondents' answers to questions vary based on a certain kind of priming, for example, the biggest hurdle isn't eliciting the response, but coding it. Models incorporating embeddings (like what we're doing here) could provide more operationalized and consistent classifications of a respondent's answers, and offload very tedious work that may otherwise be undermined by qualitative coding issues. \n",
        "\n",
        "Let's start experimenting with some models, and see what we can get rolling here."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fVULmHxBRkb-"
      },
      "source": [
        "### MODEL 1: Basic LSTM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first model we're gonna run is going to be a fairly basic `LSTM()` model. We will first need to tokenize our reviews and construct word embeddings. The code provided in the example notebook will help us get these embeddings computed, but I'm going to tweak some of the parameters here and experiment. We're going to run with a max sample length of 60, and work with 10 output features to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fs6KWo56SrUI"
      },
      "outputs": [],
      "source": [
        "# Read libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Set variables for preprocessor and later code\n",
        "vocab_size = 10000\n",
        "max_length = 60\n",
        "features = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-ITxKtfToLZ",
        "outputId": "8c7f1a36-c7b2-42c0-830f-de7c0300f245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6920, 60)\n",
            "(1821, 60)\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary from training text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Use the packaged preprocessor; I'm going to adjust the maxlen 100\n",
        "def preprocessor(data, maxlen=50, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train, maxlen=max_length, max_words = vocab_size).shape)\n",
        "print(preprocessor(X_test, maxlen=max_length, max_words = vocab_size).shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to run a fairly minimal example here, with an embeddings layer feeding into an LSTM layer with a recurrent dropout of 0.2 to try and improve its generalizability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puZhY2N7SRFQ",
        "outputId": "18c8804b-06d3-433e-8fe6-9eec446a40cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_39 (Embedding)    (None, 60, 10)            100000    \n",
            "                                                                 \n",
            " lstm_31 (LSTM)              (None, 64)                19200     \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 119,330\n",
            "Trainable params: 119,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "vocab_size = 10000\n",
        "\n",
        "# Define the model architecture\n",
        "model_one = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=features, input_length=max_length),\n",
        "    LSTM(64, recurrent_dropout=0.2),\n",
        "    Flatten(),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_one.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model_one.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6RV2qiEVpeD",
        "outputId": "8cbbba66-d052-4fb6-d790-dd87837f5213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 5s 22ms/step - loss: 0.6592 - acc: 0.6176 - val_loss: 0.8341 - val_acc: 0.3331\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.4884 - acc: 0.7859 - val_loss: 0.5937 - val_acc: 0.7225\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.2839 - acc: 0.8882 - val_loss: 0.5519 - val_acc: 0.7601\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.1715 - acc: 0.9370 - val_loss: 0.7125 - val_acc: 0.7233\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.1062 - acc: 0.9630 - val_loss: 0.9230 - val_acc: 0.7052\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.0707 - acc: 0.9751 - val_loss: 0.8267 - val_acc: 0.7211\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.0478 - acc: 0.9868 - val_loss: 0.8071 - val_acc: 0.7551\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 4s 22ms/step - loss: 0.0364 - acc: 0.9895 - val_loss: 1.0680 - val_acc: 0.7247\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 4s 23ms/step - loss: 0.0241 - acc: 0.9940 - val_loss: 1.1535 - val_acc: 0.7370\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 4s 22ms/step - loss: 0.0219 - acc: 0.9940 - val_loss: 1.3644 - val_acc: 0.7319\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21dfe1057f0>"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model_one.fit(preprocessor(X_train, maxlen=max_length, max_words = vocab_size), y_train, validation_split=0.2, epochs=10, batch_size=32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the basic stats, it looks like we did okay for a fairly low-context model. Our validation accuracy got to around 75% at one point; however, the validation loss increased substantially with each epoch, which means we might want to consider some form of monitoring for that. When we submit this to the challenge we'll be able to see how that affected its overall peformance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5r5SqQRrbne6",
        "outputId": "7729b141-0f49-4d3d-f09f-6067460f5ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your preprocessor is now saved to 'preprocessor.zip'\n",
            "AI Model Share login credentials set successfully.\n",
            "57/57 [==============================] - 1s 7ms/step\n",
            "\n",
            "Your model has been submitted as model version 383\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "# Save Preprocessor\n",
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"\")\n",
        "\n",
        "\n",
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model_one, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_one.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "\n",
        "# Login\n",
        "from aimodelshare.aws import set_credentials\n",
        "    \n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)\n",
        "\n",
        "\n",
        "#Instantiate Competition\n",
        "\n",
        "mycompetition= ai.Competition(apiurl)\n",
        "\n",
        "\n",
        "#Submit model_one\n",
        "prediction_column_index=model_one.predict(preprocessor(X_test, maxlen=max_length, max_words = vocab_size)).argmax(axis=1)\n",
        "\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "\n",
        "mycompetition.submit_model(model_filepath = \"model_one.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can check the leaderboard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>ml_framework</th>\n",
              "      <th>transfer_learning</th>\n",
              "      <th>deep_learning</th>\n",
              "      <th>model_type</th>\n",
              "      <th>depth</th>\n",
              "      <th>num_params</th>\n",
              "      <th>...</th>\n",
              "      <th>softmax_act</th>\n",
              "      <th>tanh_act</th>\n",
              "      <th>relu_act</th>\n",
              "      <th>loss</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>memory_size</th>\n",
              "      <th>team</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.780461</td>\n",
              "      <td>0.779924</td>\n",
              "      <td>0.783343</td>\n",
              "      <td>0.780516</td>\n",
              "      <td>keras</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>Sequential</td>\n",
              "      <td>4.0</td>\n",
              "      <td>119330.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>str</td>\n",
              "      <td>Adam</td>\n",
              "      <td>478192.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jer2240</td>\n",
              "      <td>2023-04-18 00:37:25.382011</td>\n",
              "      <td>383</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     accuracy  f1_score  precision    recall ml_framework transfer_learning  \\\n",
              "120  0.780461  0.779924   0.783343  0.780516        keras               NaN   \n",
              "\n",
              "    deep_learning  model_type  depth  num_params  ...  softmax_act  tanh_act  \\\n",
              "120          True  Sequential    4.0    119330.0  ...          1.0       1.0   \n",
              "\n",
              "     relu_act  loss  optimizer  memory_size  team  username  \\\n",
              "120       NaN   str       Adam     478192.0   NaN   jer2240   \n",
              "\n",
              "                      timestamp  version  \n",
              "120  2023-04-18 00:37:25.382011      383  \n",
              "\n",
              "[1 rows x 35 columns]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get leaderboard\n",
        "\n",
        "condensed_leaderboard = mycompetition.get_leaderboard()\n",
        "\n",
        "condensed_leaderboard[condensed_leaderboard['version'] == 383]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This actually isn't bad for a first attempt, but we hopefully could improve on this a bit and get our accuracy into the 80s. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cWvGF_eXXNae"
      },
      "source": [
        "### Model 2: Adding Layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like with models already run in this class, we can take a convolution approach to these models to try and extract more information from the embeddings. I'm going to replicate the structure we used previously, with some stacked `Conv1D` layers feeding into `MaxPooling` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5PX3NZ6XRvG",
        "outputId": "53ee0f9d-17a1-4aa1-a084-f16ac64c571a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_41 (Embedding)    (None, 60, 10)            100000    \n",
            "                                                                 \n",
            " conv1d_226 (Conv1D)         (None, 60, 32)            2272      \n",
            "                                                                 \n",
            " conv1d_227 (Conv1D)         (None, 60, 32)            7200      \n",
            "                                                                 \n",
            " max_pooling1d_101 (MaxPooli  (None, 12, 32)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_228 (Conv1D)         (None, 12, 64)            14400     \n",
            "                                                                 \n",
            " conv1d_229 (Conv1D)         (None, 12, 64)            28736     \n",
            "                                                                 \n",
            " global_max_pooling1d_14 (Gl  (None, 64)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 152,738\n",
            "Trainable params: 152,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model_two = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=features, input_length=max_length),\n",
        "    Conv1D(filters=32, kernel_size=7, padding='same', activation='relu'),\n",
        "    Conv1D(filters=32, kernel_size=7, padding='same', activation='relu'),\n",
        "    MaxPooling1D(5),\n",
        "    Conv1D(filters=64, kernel_size=7, padding='same', activation='relu'),\n",
        "    Conv1D(filters=64, kernel_size=7, padding='same', activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_two.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "plateau_check = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "\n",
        "model_two.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not a ridiculous amount of depth here, but let's see how it goes. Two other adjustments here: we're going to increase our epochs, add a plateau_check callback, and fix our optimizer to be `RMSprop` instead of `adam`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuLmBirrX_sl",
        "outputId": "095ffbf1-f36e-4da4-a8fd-6b37f90c6e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "173/173 [==============================] - 2s 7ms/step - loss: 0.6468 - acc: 0.6263 - val_loss: 0.7978 - val_acc: 0.5585 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4722 - acc: 0.7751 - val_loss: 0.9283 - val_acc: 0.5795 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.3426 - acc: 0.8544 - val_loss: 0.8179 - val_acc: 0.6488 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.2550 - acc: 0.8932 - val_loss: 0.4468 - val_acc: 0.8129 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.2044 - acc: 0.9182 - val_loss: 0.5538 - val_acc: 0.7803 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.1570 - acc: 0.9391 - val_loss: 0.4760 - val_acc: 0.8230 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "169/173 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9525\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.1248 - acc: 0.9520 - val_loss: 1.0287 - val_acc: 0.6936 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0733 - acc: 0.9762 - val_loss: 0.9374 - val_acc: 0.7529 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.0677 - acc: 0.9772 - val_loss: 1.0515 - val_acc: 0.7377 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "171/173 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9784\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0644 - acc: 0.9785 - val_loss: 0.9784 - val_acc: 0.7594 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.0593 - acc: 0.9812 - val_loss: 0.9802 - val_acc: 0.7594 - lr: 1.0000e-05\n",
            "Epoch 12/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0589 - acc: 0.9816 - val_loss: 0.9975 - val_acc: 0.7594 - lr: 1.0000e-05\n",
            "Epoch 13/20\n",
            "167/173 [===========================>..] - ETA: 0s - loss: 0.0571 - acc: 0.9826\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0586 - acc: 0.9819 - val_loss: 0.9505 - val_acc: 0.7659 - lr: 1.0000e-05\n",
            "Epoch 14/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0582 - acc: 0.9818 - val_loss: 0.9547 - val_acc: 0.7652 - lr: 1.0000e-06\n",
            "Epoch 15/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0582 - acc: 0.9818 - val_loss: 0.9580 - val_acc: 0.7645 - lr: 1.0000e-06\n",
            "Epoch 16/20\n",
            "170/173 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9818\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0582 - acc: 0.9818 - val_loss: 0.9599 - val_acc: 0.7645 - lr: 1.0000e-06\n",
            "Epoch 17/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0581 - acc: 0.9818 - val_loss: 0.9602 - val_acc: 0.7645 - lr: 1.0000e-07\n",
            "Epoch 18/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0581 - acc: 0.9818 - val_loss: 0.9605 - val_acc: 0.7645 - lr: 1.0000e-07\n",
            "Epoch 19/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.0581 - acc: 0.9818\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0581 - acc: 0.9818 - val_loss: 0.9607 - val_acc: 0.7645 - lr: 1.0000e-07\n",
            "Epoch 20/20\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.0581 - acc: 0.9818 - val_loss: 0.9608 - val_acc: 0.7645 - lr: 1.0000e-08\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21de539e2b0>"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_two.fit(preprocessor(X_train, maxlen=max_length, max_words = vocab_size), y_train, validation_split=0.2, epochs=20, batch_size=32, callbacks=[plateau_check])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like we reached an upper limit in our validation accuracy of around 76.45% within our split; I also wish I had put in a feature to recall the best weights. At any rate, let's submit and see how it performed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PAiwMpT5jedB",
        "outputId": "b390ac00-1c4b-4941-c2f0-dc25530b652e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 3ms/step\n",
            "\n",
            "Your model has been submitted as model version 386\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "# Save model_two\n",
        "\n",
        "onnx_model = model_to_onnx(model_two, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_two.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "\n",
        "\n",
        "#Submit model_two\n",
        "prediction_column_index=model_two.predict(preprocessor(X_test, maxlen=max_length, max_words = vocab_size)).argmax(axis=1)\n",
        "\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "\n",
        "mycompetition.submit_model(model_filepath = \"model_two.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And to see how we did:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>ml_framework</th>\n",
              "      <th>transfer_learning</th>\n",
              "      <th>deep_learning</th>\n",
              "      <th>model_type</th>\n",
              "      <th>depth</th>\n",
              "      <th>num_params</th>\n",
              "      <th>...</th>\n",
              "      <th>softmax_act</th>\n",
              "      <th>tanh_act</th>\n",
              "      <th>relu_act</th>\n",
              "      <th>loss</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>memory_size</th>\n",
              "      <th>team</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.793633</td>\n",
              "      <td>0.792933</td>\n",
              "      <td>0.797806</td>\n",
              "      <td>0.793698</td>\n",
              "      <td>keras</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>Sequential</td>\n",
              "      <td>8.0</td>\n",
              "      <td>152738.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>str</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>612544.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jer2240</td>\n",
              "      <td>2023-04-18 00:54:12.859599</td>\n",
              "      <td>386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  f1_score  precision    recall ml_framework transfer_learning  \\\n",
              "69  0.793633  0.792933   0.797806  0.793698        keras               NaN   \n",
              "\n",
              "   deep_learning  model_type  depth  num_params  ...  softmax_act  tanh_act  \\\n",
              "69          True  Sequential    8.0    152738.0  ...          1.0       NaN   \n",
              "\n",
              "    relu_act  loss  optimizer  memory_size  team  username  \\\n",
              "69       4.0   str    RMSprop     612544.0   NaN   jer2240   \n",
              "\n",
              "                     timestamp  version  \n",
              "69  2023-04-18 00:54:12.859599      386  \n",
              "\n",
              "[1 rows x 35 columns]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "condensed_leaderboard = mycompetition.get_leaderboard()\n",
        "\n",
        "condensed_leaderboard[condensed_leaderboard['version'] == 386]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This was a substantial improvement over our last model, with our accuracy scraping 80% and putting us firmly into the upper quadrant of the leaderboard."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3iUQWpOPaxFE"
      },
      "source": [
        "### Model 3: Transferring Weights that Fit Like a Glove"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like with the image classification problems, we can also build models with pre-trained weights. For this challenge, we're going to bring in the \"Glove\" weights as provided through the example notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx5DTam3awuI",
        "outputId": "527d5a54-5efd-4608-cc91-4e6f45dd121a"
      },
      "outputs": [],
      "source": [
        "### Read in pre-trained Glove weights\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "glove_dir = os.getcwd()\n",
        "\n",
        "\n",
        "# Set the URL of the zip file you want to download\n",
        "url = \"http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\"\n",
        "\n",
        "# Download the zip file and read its contents into memory\n",
        "r = requests.get(url)\n",
        "zip_contents = io.BytesIO(r.content)\n",
        "\n",
        "# Create a ZipFile object from the in-memory contents of the zip file\n",
        "zip_file = zipfile.ZipFile(zip_contents)\n",
        "\n",
        "# Extract all the files in the zip file to a folder\n",
        "zip_file.extractall(glove_dir)\n",
        "\n",
        "# Close the ZipFile object\n",
        "zip_file.close()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then need to rebuild an embeddings index for the new weights, like we did from our initial vectorization of the competition data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gwFPgTBdkfr",
        "outputId": "44ec2465-8bcb-45a2-a49e-73f7b231d51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ],
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this model, I'm going to work with the 200 feature version of the embedding weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hC9v2erreD6G"
      },
      "outputs": [],
      "source": [
        "# Build embedding matrix\n",
        "embedding_dim = 200 # change if you use txt files using larger number of features\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < vocab_size:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alright, let's get things fed into the model. I really want to keep working with the `LSTM` to get practice with optimizing it, so I am going to make some more tweaks. First, I'm going to add a checkpoint to keep the best performing weights in terms of validation accuracy. We're also going to increase the number of units and keep the whole sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_63 (Embedding)    (None, 60, 100)           1000000   \n",
            "                                                                 \n",
            " lstm_59 (LSTM)              (None, 128)               117248    \n",
            "                                                                 \n",
            " flatten_34 (Flatten)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,117,506\n",
            "Trainable params: 117,506\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model_three_transfer = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length,\n",
        "              weights=[embedding_matrix], trainable=False),\n",
        "    LSTM(128, recurrent_dropout=0.2),\n",
        "    Flatten(),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_three_transfer.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# add checkpoints\n",
        "checkpoint = ModelCheckpoint('best_model_three.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "model_three_transfer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWhwxB54iBpy",
        "outputId": "414d4564-132c-4808-d23c-a46d820ba403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.6922\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84393, saving model to best_model_three.h5\n",
            "173/173 [==============================] - 8s 36ms/step - loss: 0.5813 - accuracy: 0.6922 - val_loss: 0.4689 - val_accuracy: 0.8439\n",
            "Epoch 2/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.7608\n",
            "Epoch 2: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4999 - accuracy: 0.7608 - val_loss: 0.7041 - val_accuracy: 0.6033\n",
            "Epoch 3/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.7809\n",
            "Epoch 3: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.4589 - accuracy: 0.7809 - val_loss: 0.5365 - val_accuracy: 0.7283\n",
            "Epoch 4/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.7852\n",
            "Epoch 4: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.4366 - accuracy: 0.7852 - val_loss: 0.7248 - val_accuracy: 0.5824\n",
            "Epoch 5/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4086 - accuracy: 0.8118\n",
            "Epoch 5: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.4086 - accuracy: 0.8118 - val_loss: 0.6275 - val_accuracy: 0.6611\n",
            "Epoch 6/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8242\n",
            "Epoch 6: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.3796 - accuracy: 0.8242 - val_loss: 0.4414 - val_accuracy: 0.8027\n",
            "Epoch 7/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.8358\n",
            "Epoch 7: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.3572 - accuracy: 0.8358 - val_loss: 0.4183 - val_accuracy: 0.8158\n",
            "Epoch 8/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8540\n",
            "Epoch 8: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.3306 - accuracy: 0.8540 - val_loss: 0.4566 - val_accuracy: 0.7962\n",
            "Epoch 9/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.2991 - accuracy: 0.8690\n",
            "Epoch 9: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.2991 - accuracy: 0.8690 - val_loss: 0.6326 - val_accuracy: 0.7392\n",
            "Epoch 10/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.8815\n",
            "Epoch 10: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.2670 - accuracy: 0.8815 - val_loss: 0.7908 - val_accuracy: 0.6828\n",
            "Epoch 11/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.2395 - accuracy: 0.8961\n",
            "Epoch 11: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.2395 - accuracy: 0.8961 - val_loss: 0.4834 - val_accuracy: 0.8223\n",
            "Epoch 12/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9133\n",
            "Epoch 12: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.2112 - accuracy: 0.9133 - val_loss: 0.4967 - val_accuracy: 0.7962\n",
            "Epoch 13/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.9212\n",
            "Epoch 13: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.1888 - accuracy: 0.9212 - val_loss: 0.6535 - val_accuracy: 0.7457\n",
            "Epoch 14/20\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.1611 - accuracy: 0.9357\n",
            "Epoch 14: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.1608 - accuracy: 0.9359 - val_loss: 0.6007 - val_accuracy: 0.7948\n",
            "Epoch 15/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9451\n",
            "Epoch 15: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.1403 - accuracy: 0.9451 - val_loss: 0.7056 - val_accuracy: 0.7645\n",
            "Epoch 16/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9547\n",
            "Epoch 16: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.1162 - accuracy: 0.9547 - val_loss: 0.9180 - val_accuracy: 0.7363\n",
            "Epoch 17/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9626\n",
            "Epoch 17: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.1049 - accuracy: 0.9626 - val_loss: 0.7821 - val_accuracy: 0.7645\n",
            "Epoch 18/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9693\n",
            "Epoch 18: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.0874 - accuracy: 0.9693 - val_loss: 1.1744 - val_accuracy: 0.6676\n",
            "Epoch 19/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9738\n",
            "Epoch 19: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.0751 - accuracy: 0.9738 - val_loss: 1.1034 - val_accuracy: 0.7283\n",
            "Epoch 20/20\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9747\n",
            "Epoch 20: val_accuracy did not improve from 0.84393\n",
            "173/173 [==============================] - 5s 31ms/step - loss: 0.0719 - accuracy: 0.9747 - val_loss: 0.9086 - val_accuracy: 0.7673\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21e1dfd4610>"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_three_transfer.fit(preprocessor(X_train, maxlen=max_length, max_words = vocab_size), y_train, validation_split=0.2, epochs=20, batch_size=32, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "vNbBUIP7qbwd",
        "outputId": "ae02a1db-8659-4494-de34-0dc4a000320f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 8ms/step\n",
            "\n",
            "Your model has been submitted as model version 392\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Save model_three\n",
        "\n",
        "model_three_best = load_model(\"best_model_three.h5\")\n",
        "\n",
        "onnx_model = model_to_onnx(model_three_best, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_three.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "\n",
        "\n",
        "#Submit model_two\n",
        "prediction_column_index=model_three_transfer.predict(preprocessor(X_test, maxlen=max_length, max_words = vocab_size)).argmax(axis=1)\n",
        "\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "\n",
        "mycompetition.submit_model(model_filepath = \"model_three.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>ml_framework</th>\n",
              "      <th>transfer_learning</th>\n",
              "      <th>deep_learning</th>\n",
              "      <th>model_type</th>\n",
              "      <th>depth</th>\n",
              "      <th>num_params</th>\n",
              "      <th>...</th>\n",
              "      <th>softmax_act</th>\n",
              "      <th>tanh_act</th>\n",
              "      <th>relu_act</th>\n",
              "      <th>loss</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>memory_size</th>\n",
              "      <th>team</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.798024</td>\n",
              "      <td>0.797847</td>\n",
              "      <td>0.799148</td>\n",
              "      <td>0.798058</td>\n",
              "      <td>keras</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>Sequential</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1117506.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>function</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>4470896.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jer2240</td>\n",
              "      <td>2023-04-18 01:50:35.620872</td>\n",
              "      <td>392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  f1_score  precision    recall ml_framework transfer_learning  \\\n",
              "54  0.798024  0.797847   0.799148  0.798058        keras               NaN   \n",
              "\n",
              "   deep_learning  model_type  depth  num_params  ...  softmax_act  tanh_act  \\\n",
              "54          True  Sequential    4.0   1117506.0  ...          1.0       1.0   \n",
              "\n",
              "    relu_act      loss  optimizer  memory_size  team  username  \\\n",
              "54       NaN  function    RMSprop    4470896.0   NaN   jer2240   \n",
              "\n",
              "                     timestamp  version  \n",
              "54  2023-04-18 01:50:35.620872      392  \n",
              "\n",
              "[1 rows x 35 columns]"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get raw data in pandas data frame\n",
        "condensed_leaderboard = mycompetition.get_leaderboard()\n",
        "\n",
        "condensed_leaderboard[condensed_leaderboard['version'] == 392]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A little more improvement! After some discussion, we will hopefully have a better idea on what to tweak next."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 4: Mixing and Matching Architectures"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After talking with my teammates, we all found a few things to be true across our projects. One, models using `Conv1D` layers appeared to be performing the best, followed closely behind by my `LSTM` model with Glove weights. Two, paying attention to callbacks is very important to maximize your model's performance. And lastly, `RMSprop` is deinifitely the way to go for an optimizer. \n",
        "\n",
        "Interestingly, we had mixed success using the pre-trained weights. One teammate found them to be very effective, another said they re-ran their Glove model a few times and never got great results, albeit with the same number of available weights. Something worth exploring here is whether fewer weights does serve to improve accuracy. We also generally weren't sure of whether the number of embedding outputs and LSTM/RNN units was something to increase or decrease, since we used the same values for the most part. We all agreed that adding more units would certainly help depth and immediate accuracy, but could just be opening up the opportunity for overfitting. \n",
        "\n",
        "All of us moved forward with the same few parameters in mind. For one, we were all going to keep adding `Dropout` layers or parameters, since they seemed to be key to achieving consistent generalizability. Additionally, concluding a `Conv1D`-based model with a `GlobalMaxPooling` layer performed very well. I also wanted to experiment with `EarlyStopping`, as this proved very effective for one teammate in making sure they didn't incur too much validation loss.\n",
        "\n",
        "Model four's major architectural change is going to be greater depth from stacking together more `Conv1D` layers. I'm also going to work in the early stopping, and go back to our original competition data to embeddings weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_67 (Embedding)    (None, 60, 7)             70000     \n",
            "                                                                 \n",
            " conv1d_254 (Conv1D)         (None, 60, 32)            1152      \n",
            "                                                                 \n",
            " conv1d_255 (Conv1D)         (None, 60, 32)            5152      \n",
            "                                                                 \n",
            " max_pooling1d_118 (MaxPooli  (None, 20, 32)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_256 (Conv1D)         (None, 20, 32)            5152      \n",
            "                                                                 \n",
            " conv1d_257 (Conv1D)         (None, 20, 32)            5152      \n",
            "                                                                 \n",
            " max_pooling1d_119 (MaxPooli  (None, 6, 32)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_258 (Conv1D)         (None, 6, 64)             10304     \n",
            "                                                                 \n",
            " conv1d_259 (Conv1D)         (None, 6, 64)             20544     \n",
            "                                                                 \n",
            " max_pooling1d_120 (MaxPooli  (None, 2, 64)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_260 (Conv1D)         (None, 2, 64)             20544     \n",
            "                                                                 \n",
            " conv1d_261 (Conv1D)         (None, 2, 64)             20544     \n",
            "                                                                 \n",
            " global_max_pooling1d_18 (Gl  (None, 64)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 158,674\n",
            "Trainable params: 158,674\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "max_length = 60\n",
        "embedding_dim = 7\n",
        "\n",
        "# Define the model architecture\n",
        "model_four = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
        "    Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
        "    Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
        "    MaxPooling1D(3),\n",
        "    Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
        "    Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
        "    MaxPooling1D(3),\n",
        "    Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
        "    Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
        "    MaxPooling1D(3),\n",
        "    Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
        "    Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.33),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_four.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# add checkpoints\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
        "\n",
        "model_four.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "173/173 [==============================] - 2s 8ms/step - loss: 0.6563 - accuracy: 0.6131 - val_loss: 0.7896 - val_accuracy: 0.1488\n",
            "Epoch 2/30\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.5168 - accuracy: 0.7585 - val_loss: 0.6065 - val_accuracy: 0.7478\n",
            "Epoch 3/30\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.3846 - accuracy: 0.8378 - val_loss: 0.6052 - val_accuracy: 0.7637\n",
            "Epoch 4/30\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.3012 - accuracy: 0.8806 - val_loss: 0.8545 - val_accuracy: 0.6676\n",
            "Epoch 5/30\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.2421 - accuracy: 0.9082 - val_loss: 0.6388 - val_accuracy: 0.7536\n",
            "Epoch 6/30\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.2029 - accuracy: 0.9256 - val_loss: 0.5759 - val_accuracy: 0.7760\n",
            "Epoch 7/30\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.1719 - accuracy: 0.9321 - val_loss: 0.4387 - val_accuracy: 0.8360\n",
            "Epoch 8/30\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.1429 - accuracy: 0.9469 - val_loss: 0.7678 - val_accuracy: 0.7861\n",
            "Epoch 9/30\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.1277 - accuracy: 0.9554 - val_loss: 1.4595 - val_accuracy: 0.7370\n",
            "Epoch 10/30\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.1019 - accuracy: 0.9635 - val_loss: 0.6848 - val_accuracy: 0.7645\n",
            "Epoch 11/30\n",
            "173/173 [==============================] - 1s 7ms/step - loss: 0.0915 - accuracy: 0.9671 - val_loss: 0.6910 - val_accuracy: 0.8165\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21e18e28190>"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_four.fit(preprocessor(X_train, maxlen=max_length, max_words = vocab_size), y_train, validation_split=0.2, epochs=30, batch_size=32, callbacks = [early_stopping])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks pretty good, let's see how it fares with the real stuff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 2ms/step\n",
            "\n",
            "Your model has been submitted as model version 393\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "# Save model_four\n",
        "\n",
        "\n",
        "onnx_model = model_to_onnx(model_four, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_four.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "\n",
        "\n",
        "#Submit model_four\n",
        "prediction_column_index=model_four.predict(preprocessor(X_test, maxlen=max_length, max_words = vocab_size)).argmax(axis=1)\n",
        "\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "\n",
        "mycompetition.submit_model(model_filepath = \"model_four.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>ml_framework</th>\n",
              "      <th>transfer_learning</th>\n",
              "      <th>deep_learning</th>\n",
              "      <th>model_type</th>\n",
              "      <th>depth</th>\n",
              "      <th>num_params</th>\n",
              "      <th>...</th>\n",
              "      <th>softmax_act</th>\n",
              "      <th>tanh_act</th>\n",
              "      <th>relu_act</th>\n",
              "      <th>loss</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>memory_size</th>\n",
              "      <th>team</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.81449</td>\n",
              "      <td>0.814457</td>\n",
              "      <td>0.814676</td>\n",
              "      <td>0.814476</td>\n",
              "      <td>keras</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>Sequential</td>\n",
              "      <td>15.0</td>\n",
              "      <td>158674.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.0</td>\n",
              "      <td>str</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>637376.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jer2240</td>\n",
              "      <td>2023-04-18 02:08:08.549804</td>\n",
              "      <td>393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  f1_score  precision    recall ml_framework transfer_learning  \\\n",
              "11   0.81449  0.814457   0.814676  0.814476        keras               NaN   \n",
              "\n",
              "   deep_learning  model_type  depth  num_params  ...  softmax_act  tanh_act  \\\n",
              "11          True  Sequential   15.0    158674.0  ...          1.0       NaN   \n",
              "\n",
              "    relu_act  loss  optimizer  memory_size  team  username  \\\n",
              "11       8.0   str    RMSprop     637376.0   NaN   jer2240   \n",
              "\n",
              "                     timestamp  version  \n",
              "11  2023-04-18 02:08:08.549804      393  \n",
              "\n",
              "[1 rows x 35 columns]"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get raw data in pandas data frame\n",
        "condensed_leaderboard = mycompetition.get_leaderboard()\n",
        "\n",
        "condensed_leaderboard[condensed_leaderboard['version'] == 393]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 5: Bringing Back the Weights, Adding More Depth"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've managed to home in on a great `Conv1D` model, so what if we tried tweaking our Glove + LSTM approach to maximize performance within that architecture. We'll bring down our maximum length and go down a stage of Glove weights to 100. We'll have to rebuild the embeddings index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 50\n",
        "# Build new embedding matrix\n",
        "embedding_dim = 100 # change if you use txt files using larger number of features\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < vocab_size:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then we're going to try our redesigned `LSTM` model (reducing the units), with our adjusted Glove parameter and a slightly higher dropout rate. We're also again going to including early stopping again, with a slightly higher patience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_66\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_71 (Embedding)    (None, 50, 100)           1000000   \n",
            "                                                                 \n",
            " lstm_63 (LSTM)              (None, 32)                17024     \n",
            "                                                                 \n",
            " flatten_38 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,017,090\n",
            "Trainable params: 17,090\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model architecture\n",
        "model_five_transfer = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
        "    LSTM(32, recurrent_dropout=.5),\n",
        "    Flatten(),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_five_transfer.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# add checkpoints\n",
        "early_stoppping = EarlyStopping(patience=6, monitor='val_loss', verbose=1)\n",
        "\n",
        "model_five_transfer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.6658\n",
            "Epoch 1: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 4s 16ms/step - loss: 0.6045 - accuracy: 0.6658 - val_loss: 0.6973 - val_accuracy: 0.6481\n",
            "Epoch 2/10\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.5290 - accuracy: 0.7380\n",
            "Epoch 2: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.5291 - accuracy: 0.7381 - val_loss: 0.5319 - val_accuracy: 0.7955\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.7645\n",
            "Epoch 3: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.4950 - accuracy: 0.7645 - val_loss: 0.5452 - val_accuracy: 0.7616\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4683 - accuracy: 0.7771\n",
            "Epoch 4: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.4683 - accuracy: 0.7771 - val_loss: 0.5219 - val_accuracy: 0.7587\n",
            "Epoch 5/10\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.4452 - accuracy: 0.7887\n",
            "Epoch 5: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.4454 - accuracy: 0.7888 - val_loss: 0.6244 - val_accuracy: 0.6705\n",
            "Epoch 6/10\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.4330 - accuracy: 0.7972\n",
            "Epoch 6: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.4327 - accuracy: 0.7979 - val_loss: 0.5912 - val_accuracy: 0.7052\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.8067\n",
            "Epoch 7: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.4138 - accuracy: 0.8067 - val_loss: 0.5023 - val_accuracy: 0.7760\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.8235\n",
            "Epoch 8: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.3947 - accuracy: 0.8235 - val_loss: 0.5782 - val_accuracy: 0.7088\n",
            "Epoch 9/10\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8272\n",
            "Epoch 9: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.3842 - accuracy: 0.8271 - val_loss: 0.5067 - val_accuracy: 0.7803\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.8322\n",
            "Epoch 10: val_accuracy did not improve from 0.80347\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.3758 - accuracy: 0.8322 - val_loss: 0.7238 - val_accuracy: 0.6647\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21e9606ba60>"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_five_transfer.fit(preprocessor(X_train, maxlen=max_length, max_words = vocab_size), y_train, validation_split=0.2, epochs=10, batch_size=32, callbacks = [early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 4ms/step\n",
            "\n",
            "Your model has been submitted as model version 394\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "# Save model_five\n",
        "\n",
        "onnx_model = model_to_onnx(model_five_transfer, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_five.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "\n",
        "#Submit model_five\n",
        "prediction_column_index=model_five_transfer.predict(preprocessor(X_test, maxlen=max_length, max_words = vocab_size)).argmax(axis=1)\n",
        "\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "\n",
        "mycompetition.submit_model(model_filepath = \"model_five.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>ml_framework</th>\n",
              "      <th>transfer_learning</th>\n",
              "      <th>deep_learning</th>\n",
              "      <th>model_type</th>\n",
              "      <th>depth</th>\n",
              "      <th>num_params</th>\n",
              "      <th>...</th>\n",
              "      <th>softmax_act</th>\n",
              "      <th>tanh_act</th>\n",
              "      <th>relu_act</th>\n",
              "      <th>loss</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>memory_size</th>\n",
              "      <th>team</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.780461</td>\n",
              "      <td>0.77665</td>\n",
              "      <td>0.801346</td>\n",
              "      <td>0.780605</td>\n",
              "      <td>keras</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>Sequential</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1017090.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>str</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>4069232.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jer2240</td>\n",
              "      <td>2023-04-18 02:23:42.259191</td>\n",
              "      <td>394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  f1_score  precision    recall ml_framework transfer_learning  \\\n",
              "97  0.780461   0.77665   0.801346  0.780605        keras               NaN   \n",
              "\n",
              "   deep_learning  model_type  depth  num_params  ...  softmax_act  tanh_act  \\\n",
              "97          True  Sequential    4.0   1017090.0  ...          1.0       1.0   \n",
              "\n",
              "    relu_act  loss  optimizer  memory_size  team  username  \\\n",
              "97       NaN   str    RMSprop    4069232.0   NaN   jer2240   \n",
              "\n",
              "                     timestamp  version  \n",
              "97  2023-04-18 02:23:42.259191      394  \n",
              "\n",
              "[1 rows x 35 columns]"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get leaderboard\n",
        "\n",
        "condensed_leaderboard = mycompetition.get_leaderboard()\n",
        "\n",
        "condensed_leaderboard[condensed_leaderboard['version'] == 394]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is definitely our best `LSTM` yet, but it still has room for greater depth, and in turn, perhaps better external accuracy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Model: Packing It All In"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this last model, we're going to try stacking up LSTM depth while also using callbacks to help maximize our training time and reduce overfitting. We're going to use the Glove weights again, which will feed into progressively smaller LSTM layers, and then include a plateau-check to make sure we don't get stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_76 (Embedding)    (None, 50, 100)           1000000   \n",
            "                                                                 \n",
            " lstm_75 (LSTM)              (None, 50, 64)            42240     \n",
            "                                                                 \n",
            " lstm_76 (LSTM)              (None, 50, 32)            12416     \n",
            "                                                                 \n",
            " lstm_77 (LSTM)              (None, 16)                3136      \n",
            "                                                                 \n",
            " flatten_43 (Flatten)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,057,826\n",
            "Trainable params: 57,826\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "model_final = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
        "    \n",
        "    LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
        "    LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
        "    LSTM(16),\n",
        "    Flatten(),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_final.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add plateau-checker\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "model_final.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "173/173 [==============================] - 11s 43ms/step - loss: 0.6157 - accuracy: 0.6552 - val_loss: 0.7544 - val_accuracy: 0.5788 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.5493 - accuracy: 0.7141 - val_loss: 0.7630 - val_accuracy: 0.6402 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.5169 - accuracy: 0.7467 - val_loss: 0.5541 - val_accuracy: 0.7673 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.4910 - accuracy: 0.7581 - val_loss: 0.7923 - val_accuracy: 0.5289 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.4724 - accuracy: 0.7758 - val_loss: 0.6302 - val_accuracy: 0.6792 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.4501 - accuracy: 0.7890 - val_loss: 0.4965 - val_accuracy: 0.7890 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.4363 - accuracy: 0.7939 - val_loss: 0.5321 - val_accuracy: 0.7471 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.4225 - accuracy: 0.8015 - val_loss: 0.4737 - val_accuracy: 0.7782 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.4054 - accuracy: 0.8102 - val_loss: 0.4485 - val_accuracy: 0.8049 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.3934 - accuracy: 0.8156 - val_loss: 0.6086 - val_accuracy: 0.7413 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.3840 - accuracy: 0.8264 - val_loss: 0.8290 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.3752 - accuracy: 0.8212 - val_loss: 0.6467 - val_accuracy: 0.6922 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.3489 - accuracy: 0.8407 - val_loss: 0.4812 - val_accuracy: 0.7948 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8438Restoring model weights from the end of the best epoch: 9.\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.3432 - accuracy: 0.8438 - val_loss: 0.5955 - val_accuracy: 0.7204 - lr: 0.0010\n",
            "Epoch 14: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21e97ab0040>"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_final.fit(preprocessor(X_train, maxlen=max_length, max_words = vocab_size), y_train, validation_split=0.2, epochs=30, batch_size=32, callbacks = [early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 1s 11ms/step\n",
            "\n",
            "Your model has been submitted as model version 396\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "# Save final model\n",
        "\n",
        "onnx_model = model_to_onnx(model_final, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model_final.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "\n",
        "#Submit model_two\n",
        "prediction_column_index=model_final.predict(preprocessor(X_test, maxlen=max_length, max_words = vocab_size)).argmax(axis=1)\n",
        "\n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "\n",
        "mycompetition.submit_model(model_filepath = \"model_final.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>ml_framework</th>\n",
              "      <th>transfer_learning</th>\n",
              "      <th>deep_learning</th>\n",
              "      <th>model_type</th>\n",
              "      <th>depth</th>\n",
              "      <th>num_params</th>\n",
              "      <th>...</th>\n",
              "      <th>softmax_act</th>\n",
              "      <th>tanh_act</th>\n",
              "      <th>relu_act</th>\n",
              "      <th>loss</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>memory_size</th>\n",
              "      <th>team</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.78595</td>\n",
              "      <td>0.78594</td>\n",
              "      <td>0.785985</td>\n",
              "      <td>0.785943</td>\n",
              "      <td>keras</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>Sequential</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1057826.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>str</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>4232976.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jer2240</td>\n",
              "      <td>2023-04-18 03:05:23.562487</td>\n",
              "      <td>396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     accuracy  f1_score  precision    recall ml_framework transfer_learning  \\\n",
              "107   0.78595   0.78594   0.785985  0.785943        keras               NaN   \n",
              "\n",
              "    deep_learning  model_type  depth  num_params  ...  softmax_act  tanh_act  \\\n",
              "107          True  Sequential    6.0   1057826.0  ...          1.0       3.0   \n",
              "\n",
              "     relu_act  loss  optimizer  memory_size  team  username  \\\n",
              "107       NaN   str    RMSprop    4232976.0   NaN   jer2240   \n",
              "\n",
              "                      timestamp  version  \n",
              "107  2023-04-18 03:05:23.562487      396  \n",
              "\n",
              "[1 rows x 35 columns]"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get leaderboard\n",
        "\n",
        "condensed_leaderboard = mycompetition.get_leaderboard()\n",
        "\n",
        "condensed_leaderboard[condensed_leaderboard['version'] == 396]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We actually lost some performance here, and I think that's because the design was a bit flawed. The EarlyStopping I think impeded the `ReduceOnPlateau` utility, and the dropout in our two layers I think may have also provided diminishing returns over time with regards to generalizability. With more time, I would be curious to see how running these tests without the Dropout would have gone. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusions\n",
        "\n",
        "\n",
        "There is a lot to be learned from this little experiments when it comes to using text data with a classifier. \n",
        "\n",
        "First, although it's definitely a valuable approach worth exploring, the complexity of an `LSTM` will not necessarily be the best approach for all language problems. The sequential aspect of the text data may not have been that important for classifying the overall sentiment of each review/headline. Everyone on my team had fairly effective Conv1D models and did not find an LSTM structure that was able to improve on it, though maybe more time to exlore hyperparameters would have revealed something in the smaller details.\n",
        "\n",
        "Additonally, all of these models benefitted greatly from callbacks and the \"in-between\" layers/parameters. For example, adding `Dropoff` both in the `LSTM` and the `Conv1D` models were essential for good generalizing to the competition Test data. Similarly, structuring the `Conv1D` models around a `GlobalMaxPooling` instead of a `Flatten()` proved to bring out greater performance. Though this was a valuable lesson from the previous project as well, these test models really demonstrated how important it is to not just get caught up in feeding the depth of your model, but also *extending* this depth to be as flexible as possible. \n",
        "\n",
        "Generally, this task was far more demanding of a design that was well thought-through. Working with Dropoff, keeping sequences, and learning rate reducers didn't just automatically make the model better; in the case of learning rate, for example, it may have conflicted with the function of `EarlyStopping` and prevented that from being as useful as possible. EarlyStopping was also a valuable design lesson, as it required a bit of tweaking to reach the balance between overfit prevention and sacrificed training time. Given that there are differing approaches you could take with representing this data, being mindful of these kinds of trade-offs will help you construct a model architecture that isn't subtley working against itself. With my final model, for example, I actually re-ran it's initial training period a few times to see how different EarlyStopping timings affected its performance; I observed that there definitely was a distinct possibility that too early of a cutoff will prevent your model from learning everything it needs to know about your data. \n",
        "\n",
        "I have a background in linguistics, and spent a lot of time learning much more qualitative attempts at modeling language. Implementing these models was fascinating because it made me consider both how much information you could extract from words alone, but also what you're still losing by being constrained to them. I can definitely see the advantages of an `LSTM` when it comes to trying to capture consistent multi-word patterns in how we express appreciation or contempt. But I wonder if capturing these patterns may sometimes create noise, especially when you have fairly condensed text extracts like the ones we were training with. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
